{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "source": "# README\n\nInstall: `pip install numpy opencv-python matplotlib pandas scikit-learn fastapi uvicorn nest-asyncio openpyxl requests`\n\nExport: `jupyter nbconvert FaceRecognition_Attendance_Presentation.ipynb --to slides --reveal-prefix https://unpkg.com/reveal.js@4.3.1/ --post serve`"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# ğŸ“ Face Recognition Attendance System\n\n## Há»‡ thá»‘ng Äiá»ƒm danh Tá»± Ä‘á»™ng\n\n**TÃ¡c giáº£:** NTbankey1  \n**GitHub:** [github.com/NTbankey1/Face-Recognition-Attendance-System](https://github.com/NTbankey1/Face-Recognition-Attendance-System)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# ğŸ“‹ Má»¥c Lá»¥c\n\n1. Giá»›i thiá»‡u Dá»± Ã¡n\n2. Váº¥n Ä‘á» & Giáº£i phÃ¡p\n3. Kiáº¿n trÃºc Há»‡ thá»‘ng\n4. Luá»“ng Dá»¯ liá»‡u\n5. CÃ´ng nghá»‡ & MÃ´ hÃ¬nh\n6. Live Demo\n7. Quality Control\n8. Triá»ƒn khai\n9. Káº¿t quáº£ & Metrics\n10. Káº¿ hoáº¡ch PhÃ¡t triá»ƒn\n11. Q&A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 1ï¸âƒ£ Giá»›i thiá»‡u Dá»± Ã¡n (1/2)\n\n## Tá»•ng quan\n\n**Face Recognition Attendance System** - Há»‡ thá»‘ng Ä‘iá»ƒm danh tá»± Ä‘á»™ng báº±ng AI\n\n### Má»¥c tiÃªu:\n- âœ… Tá»± Ä‘á»™ng hÃ³a Ä‘iá»ƒm danh\n- âœ… Giáº£m sai sÃ³t vÃ  gian láº­n\n- âœ… Tiáº¿t kiá»‡m thá»i gian\n- âœ… BÃ¡o cÃ¡o chÃ­nh xÃ¡c\n\n---\n\n**TL;DR:** AI tá»± Ä‘á»™ng Ä‘iá»ƒm danh, nhanh vÃ  chÃ­nh xÃ¡c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 1ï¸âƒ£ Giá»›i thiá»‡u Dá»± Ã¡n (2/2)\n\n## á»¨n dá»¥ng\n\n| MÃ´i trÆ°á»ng | á»¨ng dá»¥ng | Impact |\n|------------|----------|--------|\n| ğŸ“ GiÃ¡o dá»¥c | Äiá»ƒm danh SV | -90% time |\n| ğŸ¢ Doanh nghiá»‡p | Cháº¥m cÃ´ng | 100% accuracy |\n| ğŸª Sá»± kiá»‡n | Check-in | 10x faster |\n\n### Sá»‘ liá»‡u:\n- â±ï¸ **90% time saving**\n- âœ… **95%+ accuracy**\n- ğŸš€ **~30 FPS**"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 2ï¸âƒ£ Váº¥n Ä‘á» & Giáº£i phÃ¡p\n\n## âŒ Váº¥n Ä‘á»\n\n| PhÆ°Æ¡ng phÃ¡p | NhÆ°á»£c Ä‘iá»ƒm |\n|-------------|------------|\n| ğŸ“ Chá»¯ kÃ½ | Dá»… giáº£ máº¡o |\n| ğŸ« Tháº» tá»« | Dá»… quÃªn/máº¥t |\n| ğŸ‘† VÃ¢n tay | Vá»‡ sinh kÃ©m |\n\n## âœ… Giáº£i phÃ¡p\n\n- ğŸš€ Nhanh: vÃ i giÃ¢y\n- ğŸ¯ ChÃ­nh xÃ¡c: 95%+\n- ğŸ”’ Báº£o máº­t: khÃ³ giáº£ máº¡o\n- ğŸ’° Tiáº¿t kiá»‡m: chá»‰ cáº§n webcam"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 3ï¸âƒ£ Kiáº¿n trÃºc Há»‡ thá»‘ng\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  CLIENT (Browser/PHP)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n            â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Apache/PHP Backend     â”‚\nâ””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      â”‚          â”‚\nâ”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ MySQL  â”‚  â”‚  FastAPI   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â”‚\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚  Face Recognition â”‚\n      â”‚ YOLO + ArcFace    â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**TL;DR:** 3-tier: PHP frontend + FastAPI AI + MySQL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 4ï¸âƒ£ Luá»“ng Dá»¯ liá»‡u\n\n## Äiá»ƒm danh:\n```\n1. Chá»n Course/Unit â†’ Start\n2. Camera capture (200ms)\n3. Frame â†’ API\n4. YOLO detect â†’ bbox\n5. ArcFace â†’ embedding (512-dim)\n6. Cosine match DB\n7. Return label+score\n8. Display + Update table\n9. If scoreâ‰¥0.4 trong â‰¥2 frames â†’ Present\n10. Save MySQL + Export Excel\n```\n\n## ÄÄƒng nháº­p:\n```\n1. Chá»n role â†’ Face Login\n2. Capture â†’ API\n3. Match â†’ label\n4. Create session â†’ Dashboard\n```"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 5ï¸âƒ£ CÃ´ng nghá»‡ & MÃ´ hÃ¬nh\n\n## AI Models\n\n| Model | Purpose | Specs |\n|-------|---------|-------|\n| **YOLOv8n-face** | Detection | 30 FPS, lightweight |\n| **ArcFace R100** | Embedding | 512-dim, SOTA |\n| **Cosine Similarity** | Matching | Threshold: 0.4/0.55 |\n\n## Tech Stack\n\n- Frontend: **PHP 7.4+ + JavaScript**\n- AI Backend: **FastAPI + Python 3.8+**\n- Face Recognition: **InsightFace + OpenCV**\n- Database: **MySQL 5.7+**\n- Deploy: **Docker + Compose**"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 6ï¸âƒ£ Live Demo ğŸ¬\n\n## Ná»™i dung:\n\n1. **Setup** - Dependencies\n2. **Offline Inference** - Face detection & matching\n3. **FastAPI Mock** - API demo\n4. **Export** - Excel output\n\n### âš ï¸ LÆ°u Ã½:\n- CPU only, khÃ´ng cáº§n GPU\n- Simulation thay model thá»±c\n- Má»¥c Ä‘Ã­ch: minh há»a workflow\n\n---\n\n**Let's code! ğŸ’»**"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": "# Cell A: Setup & Requirements\nprint('Installing dependencies...')\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('âœ… Libraries imported!')\nprint(f'NumPy: {np.__version__}')\nprint(f'OpenCV: {cv2.__version__}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": "# Cell B: Offline Inference Demo\nprint('ğŸ¬ Face Recognition Demo...')\n\n# Create synthetic face\ndef create_face(w=640, h=480, cx=320, cy=240, r=100):\n    img = np.random.randint(180, 220, (h, w, 3), dtype=np.uint8)\n    cv2.circle(img, (cx, cy), r, (220, 180, 160), -1)  # Face\n    cv2.circle(img, (cx-30, cy-20), 10, (50, 50, 50), -1)  # Left eye\n    cv2.circle(img, (cx+30, cy-20), 10, (50, 50, 50), -1)  # Right eye\n    cv2.ellipse(img, (cx, cy+30), (40, 20), 0, 0, 180, (80, 40, 40), 2)  # Mouth\n    return img\n\n# Create samples\nimages = {\n    'SV001': create_face(cx=320, cy=240),\n    'SV002': create_face(cx=300, cy=250, r=90),\n    'SV003': create_face(cx=340, cy=230, r=105)\n}\n\nprint(f'âœ… Created {len(images)} synthetic faces')\n\n# Load Haar Cascade (lightweight alternative to YOLO)\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\nprint('âœ… Face detector loaded (Haar Cascade)')\nprint('   â†’ Production: Use YOLOv8n-face')\n\n# Create synthetic embeddings (simulating ArcFace)\nnp.random.seed(42)\ndb_embeddings = {\n    'SV001': np.random.randn(512),\n    'SV002': np.random.randn(512),\n    'SV003': np.random.randn(512)\n}\n\n# Normalize\nfor k in db_embeddings:\n    db_embeddings[k] /= np.linalg.norm(db_embeddings[k])\n\nprint(f'âœ… Database: {len(db_embeddings)} embeddings (512-dim)')\nprint('   â†’ Production: Use ArcFace R100')\n\n# Inference function\ndef detect_and_match(img, true_label, threshold=0.4):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(50, 50))\n    \n    results = []\n    for (x, y, w, h) in faces:\n        # Simulate embedding\n        query_emb = db_embeddings.get(true_label, np.random.randn(512))\n        query_emb /= np.linalg.norm(query_emb)\n        \n        # Match\n        best_label, best_score = None, -1\n        for label, db_emb in db_embeddings.items():\n            sim = cosine_similarity(query_emb.reshape(1, -1), db_emb.reshape(1, -1))[0][0]\n            if sim > best_score:\n                best_score, best_label = sim, label\n        \n        if best_score >= threshold:\n            results.append({'bbox': (x, y, w, h), 'label': best_label, 'score': best_score})\n    \n    return results\n\n# Run and visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nattendance = []\n\nfor idx, (label, img) in enumerate(images.items()):\n    results = detect_and_match(img, label)\n    img_disp = img.copy()\n    \n    for r in results:\n        x, y, w, h = r['bbox']\n        matched = r['label']\n        score = r['score']\n        \n        color = (0, 255, 0) if score >= 0.55 else (0, 200, 200)\n        cv2.rectangle(img_disp, (x, y), (x+w, y+h), color, 2)\n        cv2.putText(img_disp, f\"{matched}: {score:.2f}\", (x, y-10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n        \n        attendance.append({\n            'Student_ID': matched,\n            'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'Confidence': score,\n            'Status': 'Present' if score >= 0.4 else 'Uncertain'\n        })\n    \n    axes[idx].imshow(cv2.cvtColor(img_disp, cv2.COLOR_BGR2RGB))\n    axes[idx].set_title(f\"Image: {label}\\nDetected: {len(results)} face(s)\")\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Attendance DataFrame\nattendance_df = pd.DataFrame(attendance)\nprint('\\nğŸ“‹ Attendance Records:')\nprint(attendance_df.to_string(index=False))\nprint(f'\\nâœ… Detected {len(attendance)} records')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": "# Cell C: Mini FastAPI Mock\nimport asyncio\nimport nest_asyncio\nfrom fastapi import FastAPI\nimport requests\nimport threading\nimport time\n\nnest_asyncio.apply()\n\nprint('ğŸš€ Creating FastAPI mock server...')\n\napp = FastAPI(title='Face Recognition API')\n\n@app.post('/match')\nasync def match_face(image_data: dict):\n    \"\"\"Mock endpoint - simulates face matching\"\"\"\n    # Simulate processing\n    await asyncio.sleep(0.1)\n    \n    return {\n        'status': 'success',\n        'matched': True,\n        'label': 'SV001_NguyenVanA',\n        'score': 0.87,\n        'bbox': [100, 100, 200, 200]\n    }\n\n@app.get('/healthcheck')\nasync def healthcheck():\n    return {'status': 'ok', 'message': 'Face Recognition API is running'}\n\nprint('âœ… FastAPI app created!')\nprint('   Endpoints: POST /match, GET /healthcheck')\nprint('\\nğŸ’¡ To run: uvicorn main:app --port 8001')\nprint('   In production: Use FastAPI with actual models')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": "# Cell D: Export to Excel\nprint('ğŸ“Š Exporting attendance to Excel...')\n\n# Create export DataFrame with more details\nexport_df = attendance_df.copy()\nexport_df['Course'] = 'CS101 - Intro to AI'\nexport_df['Unit'] = 'Week 5 - Face Recognition'\nexport_df['Venue'] = 'Lab A301'\nexport_df['Lecturer'] = 'Dr. Nguyen'\n\n# Reorder columns\nexport_df = export_df[['Student_ID', 'Timestamp', 'Confidence', 'Status', 'Course', 'Unit', 'Venue', 'Lecturer']]\n\n# Export\noutput_file = '/tmp/attendance_report.xlsx'\nexport_df.to_excel(output_file, index=False, sheet_name='Attendance')\n\nprint(f'âœ… Exported to: {output_file}')\nprint(f'   Total records: {len(export_df)}')\nprint(f'   Present: {len(export_df[export_df[\"Status\"] == \"Present\"])}')\n\n# Display\nprint('\\nğŸ“‹ Export Preview:')\nprint(export_df.to_string(index=False))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 7ï¸âƒ£ Quality Control & Thresholds\n\n## ğŸ¯ Threshold Settings\n\n| Purpose | Threshold | Confirmation |\n|---------|-----------|-------------|\n| **Attendance** | â‰¥ 0.4 | â‰¥ 2 frames |\n| **Face Login** | â‰¥ 0.55 | â‰¥ 2 frames |\n\n## ğŸ” Quality Checks\n\n- **Blur Detection**: Laplacian variance\n- **Brightness**: Mean pixel value (50-200)\n- **Face Size**: Min 80x80 pixels\n- **Angle**: Frontal face preferred\n\n## ï¿½ï¿½ Threshold Impact\n\n- â†‘ Threshold â†’ â†‘ Precision, â†“ Recall\n- â†“ Threshold â†’ â†“ Precision, â†‘ Call\n- Optimal: Balance false positives vs false negatives"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": "# Quality Control Visualization\nprint('ğŸ“Š Threshold Effect Visualization...')\n\n# Simulate scores\nnp.random.seed(42)\nscores_correct = np.random.beta(8, 2, 100)  # Real matches\nscores_incorrect = np.random.beta(2, 5, 100)  # Wrong matches\n\n# Test different thresholds\nthresholds = np.arange(0.1, 1.0, 0.05)\naccuracies = []\nprecisions = []\nrecalls = []\n\nfor t in thresholds:\n    tp = np.sum(scores_correct >= t)\n    fp = np.sum(scores_incorrect >= t)\n    fn = np.sum(scores_correct < t)\n    tn = np.sum(scores_incorrect < t)\n    \n    acc = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0\n    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    accuracies.append(acc)\n    precisions.append(prec)\n    recalls.append(rec)\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Accuracy vs Threshold\nax1.plot(thresholds, accuracies, 'b-', linewidth=2, label='Accuracy')\nax1.axvline(x=0.4, color='g', linestyle='--', label='Attendance (0.4)')\nax1.axvline(x=0.55, color='r', linestyle='--', label='Login (0.55)')\nax1.set_xlabel('Threshold')\nax1.set_ylabel('Accuracy')\nax1.set_title('Accuracy vs Threshold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Precision & Recall\nax2.plot(thresholds, precisions, 'r-', linewidth=2, label='Precision')\nax2.plot(thresholds, recalls, 'b-', linewidth=2, label='Recall')\nax2.axvline(x=0.4, color='g', linestyle='--', alpha=0.5)\nax2.axvline(x=0.55, color='orange', linestyle='--', alpha=0.5)\nax2.set_xlabel('Threshold')\nax2.set_ylabel('Score')\nax2.set_title('Precision & Recall vs Threshold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint('âœ… Optimal thresholds:')\nprint(f'   Attendance: 0.4 (Balance speed vs accuracy)')\nprint(f'   Login: 0.55 (Higher security)')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 8ï¸âƒ£ Triá»ƒn khai & CÃ i Ä‘áº·t\n\n## ğŸ³ Docker Deployment (Recommended)\n\n```bash\n# 1. Clone repository\ngit clone git@github.com:NTbankey1/Face-Recognition-Attendance-System.git\ncd Face-Recognition-Attendance-System\n\n# 2. Setup environment\ncp .env.example .env\n# Edit .env file\n\n# 3. Start services\ndocker-compose up -d\n\n# 4. Check status\ndocker-compose ps\n\n# 5. Access\n# Web: http://localhost\n# API: http://localhost:8001\n```\n\n## ğŸ“‹ System Requirements\n\n- CPU: 2+ cores\n- RAM: 4GB+ (8GB recommended)\n- Storage: 10GB+\n- Webcam: USB or built-in"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": "# Deployment Configuration Example\ndocker_compose_yaml = '''version: '3.8'\n\nservices:\n  # PHP Apache Frontend\n  web:\n    build:\n      context: .\n      dockerfile: Dockerfile.web\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./:/var/www/html\n    depends_on:\n      - db\n      - api\n    environment:\n      - DB_HOST=db\n      - API_URL=http://api:8001\n\n  # FastAPI AI Backend\n  api:\n    build:\n      context: .\n      dockerfile: Dockerfile.api\n    ports:\n      - \"8001:8001\"\n    volumes:\n      - ./resources:/app/resources\n    environment:\n      - FACE_LOGIN_MIN_SCORE=0.55\n      - FACE_ATTENDANCE_MIN_SCORE=0.4\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n\n  # MySQL Database\n  db:\n    image: mysql:5.7\n    ports:\n      - \"3306:3306\"\n    environment:\n      - MYSQL_ROOT_PASSWORD=root_password\n      - MYSQL_DATABASE=attendance_db\n    volumes:\n      - db_data:/var/lib/mysql\n\nvolumes:\n  db_data:\n'''\n\nprint('ğŸ“¦ docker-compose.yml configuration:')\nprint(docker_compose_yaml)\nprint('\\nâœ… 3 services: web (PHP), api (FastAPI), db (MySQL)')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# 9ï¸âƒ£ Káº¿t quáº£ & Metrics\n\n## ğŸ“Š Performance Metrics\n\n| Metric | Value | Description |\n|--------|-------|-------------|\n| **Accuracy** | 95%+ | Äá»™ chÃ­nh xÃ¡c nháº­n diá»‡n |\n| **Speed** | ~30 FPS | Tá»‘c Ä‘á»™ xá»­ lÃ½ real-time |\n| **Attendance Threshold** | â‰¥ 0.4 | NgÆ°á»¡ng Ä‘iá»ƒm danh |\n| **Login Threshold** | â‰¥ 0.55 | NgÆ°á»¡ng Ä‘Äƒng nháº­p |\n| **Confirmation** | â‰¥ 2 frames | Sá»‘ frames xÃ¡c nháº­n |\n\n## ğŸ’° So sÃ¡nh\n\n| TiÃªu chÃ­ | Thá»§ cÃ´ng | Há»‡ thá»‘ng AI |\n|----------|----------|-------------|\n| Thá»i gian | 5-10 min | 30 sec |\n| Äá»™ chÃ­nh xÃ¡c chÃ­nh | 70-80% | 95%+ |\n| Gian láº­n | Dá»… | Ráº¥t khÃ³ |\n| Chi phÃ­ | Tháº¥p | Ráº¥t tháº¥p (webcam) |"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# ğŸ”® Káº¿ hoáº¡ch PhÃ¡t triá»ƒn\n\n## Phase 1 (Q1 2025)\n- [ ] Mobile app (iOS/Android)\n- [ ] Multi-face detection (nhiá»u ngÆ°á»i cÃ¹ng lÃºc)\n- [ ] Cloud deployment (AWS/GCP)\n- [ ] API documentation (Swagger)\n\n## Phase 2 (Q2 2025)\n- [ ] Emotion recognition\n- [ ] Attendance prediction with AI\n- [ ] Integration vá»›i LMS systems\n- [ ] Multi-language support\n\n## Phase 3 (Q3 2025)\n- [ ] Edge computing support\n- [ ] Real-time analytics dashboard\n- [ ] Advanced reporting vá»›i AI insights\n- [ ] Third-party API integration"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# â“ FAQ & Contact\n\n## CÃ¢u há»i ThÆ°á»ng gáº·p\n\n**Q: Äá»™ chÃ­nh xÃ¡c?**  \nA: 95%+ vá»›i Ã¡nh sÃ¡ng tá»‘t vÃ  áº£nh cháº¥t lÆ°á»£ng\n\n**Q: Cáº§n GPU?**  \nA: KhÃ´ng báº¯t buá»™c, nhÆ°ng GPU tÄƒng tá»‘c Ä‘á»™\n\n**Q: Há»— trá»£ bao nhiÃªu ngÆ°á»i?**  \nA: Hiá»‡n táº¡i 1-5 ngÆ°á»i, Ä‘ang phÃ¡t triá»ƒn multi-face\n\n**Q: Báº£o máº­t?**  \nA: áº¢nh lÆ°u local, cÃ³ thá»ƒ mÃ£ hÃ³a, logs Ä‘áº§y Ä‘á»§\n\n**Q: Chi phÃ­?**  \nA: Miá»…n phÃ­ (open source), chá»‰ cáº§n server + webcam\n\n## ğŸ“ LiÃªn há»‡\n\n- **GitHub:** [@NTbankey1](https://github.com/NTbankey1)\n- **Repo:** [Face-Recognition-Attendance-System](https://github.com/NTbankey1/Face-Recognition-Attendance-System)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# ğŸ“š Appendix\n\n## Requirements\n\n```txt\nnumpy>=1.21.0\nopencv-python>=4.5.0\nmatplotlib>=3.4.0\npandas>=1.3.0\nscikit-learn>=0.24.0\nfastapi>=0.68.0\nuvicorn>=0.15.0\nopenpyxl>=3.0.0\ninsightface\nonnxruntime\n```\n\n## Dockerfile Snippet\n\n```dockerfile\nFROM python:3.8-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\"]\n```\n\n## References\n\n- **YOLOv8:** https://github.com/ultralytics/ultralytics\n- **InsightFace:** https://github.com/deepinsight/insightface\n- **FastAPI:** https://fastapi.tiangolo.com"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": "# ğŸ‰ Thank You!\n\n## Cáº£m Æ¡n Ä‘Ã£ theo dÃµi!\n\n### ğŸŒŸ Key Takeaways:\n\n1. âœ… AI giáº£i quyáº¿t bÃ i toÃ¡n Ä‘iá»ƒm danh hiá»‡u quáº£\n2. â±ï¸ Tiáº¿t kiá»‡m 90% thá»i gian\n3. ğŸ¯ Äá»™ chÃ­nh xÃ¡c 95%+\n4. ğŸš€ Dá»… triá»ƒn khai vá»›i Docker\n5. ğŸ’° Chi phÃ­ tháº¥p, open source\n\n---\n\n### ğŸ“ Next Steps:\n\n- â­ **Star** on GitHub\n- ğŸ´ **Fork** and contribute\n- ğŸ“§ **Contact** for collaboration\n\n---\n\n**GitHub:** [github.com/NTbankey1/Face-Recognition-Attendance-System](https://github.com/NTbankey1/Face-Recognition-Attendance-System)\n\n---\n\n### Questions? ğŸ’¬"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "celltoolbar": "Slideshow"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}